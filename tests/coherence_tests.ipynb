{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coherence_tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7FwlEnNvQJNG",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import urllib\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6AZPes5RSYG2"
      },
      "source": [
        "# 0. Preliminary step to get the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "loJLAa_Jf5Yq"
      },
      "source": [
        "Getting data from sharable google drive folder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VK81EVJim0Jc",
        "colab": {}
      },
      "source": [
        "# link to folder: https://drive.google.com/drive/folders/18TjiltRr8CFlx0aPcLsnKBr5iyeiQxWc?usp=sharing\n",
        "# upload data folder to your drive root folder 'My Drive' (It is the default folder)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t8mKSMpvh7Xr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "21d7c9ab-1e8f-417b-e143-8dbd90e93bda"
      },
      "source": [
        "# connect your drive to Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# after running this cell, open the url that displays below from your uni gmail account\n",
        "# copy the code that is displayed \n",
        "# paste the code into the cell below when prompted and then press enter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcelXsQe59E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = '/content/drive/My Drive/data_tm_workflow/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz8Y5ZMJcYdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the output folder in the drive (the csv files with the results of the process will be downloaded there)\n",
        "# first create the empty folder in the drive\n",
        "output_folder = '/content/drive/My Drive/tests'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUvuIpuzT8ke"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTOrDXSYUIc3"
      },
      "source": [
        "## 1.1. Creating data frame\n",
        "A dataframe is first created to keep the documents at their initial state, and the name of each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDu1_b_N59FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFg24uulSAjU",
        "colab": {}
      },
      "source": [
        "files_list = os.listdir(folder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lLD0OpiXv5Y",
        "colab": {}
      },
      "source": [
        "#insert file names into a df\n",
        "sources = pd.DataFrame(files_list, columns=['file_name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0PgOwPnqCfAe",
        "colab": {}
      },
      "source": [
        "#function to read the content of the text files\n",
        "def readTxtContent(fileName):\n",
        "  with open(folder_path + fileName, 'r') as file:\n",
        "    return ' ' + file.read().replace('\\n', ' ') + ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZEVIKHziQv2",
        "colab": {}
      },
      "source": [
        "# adding a column to the dataframe containing file content\n",
        "sources['file_content'] = sources['file_name'].apply(lambda x: readTxtContent(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BGPY4tOwXZUR"
      },
      "source": [
        "## 1.2. Adding columns for dates, publications and filtering dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDPSiREm59FV",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1. dates, publications\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSK4EAei59FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pde6XkQ3Xll1",
        "colab": {}
      },
      "source": [
        "# function to retrieve publication ref from file name\n",
        "def get_ref(file):\n",
        "  ref_match = re.findall(r'(\\w+\\d+)_\\d{4}-\\d{2}-\\d{2}_',file)\n",
        "  return ref_match[0]\n",
        "\n",
        "# function to retrieve date from file name\n",
        "def get_date(file):\n",
        "  date_match = re.findall(r'_(\\d{4}-\\d{2}-\\d{2})_',file)\n",
        "  return date_match[0]\n",
        "\n",
        "# function to retrieve year from file name\n",
        "def get_year(file):\n",
        "  year_match = re.findall(r'_(\\d{4})-\\d{2}-\\d{2}_',file)\n",
        "  return year_match[0]\n",
        "\n",
        "# function to retrieve month from file name\n",
        "def get_month(file):\n",
        "  month_match = re.findall(r'_\\d{4}-(\\d{2})-\\d{2}_',file)\n",
        "  return month_match[0]\n",
        "\n",
        "# function to retrieve day from file name\n",
        "def get_day(file):\n",
        "  month_match = re.findall(r'_\\d{4}-\\d{2}-(\\d{2})_',file)\n",
        "  return month_match[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFv7fxX2a0kn",
        "colab": {}
      },
      "source": [
        "sources['date'] = sources['file_name'].apply(lambda x: get_date(x))\n",
        "sources['year'] = sources['file_name'].apply(lambda x: get_year(x))\n",
        "sources['month'] = sources['file_name'].apply(lambda x: get_month(x))\n",
        "sources['day'] = sources['file_name'].apply(lambda x: get_day(x))\n",
        "sources['publication'] = sources['file_name'].apply(lambda x: get_ref(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0OeZyeG59Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add publication names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on1YgIf359Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pub_name(pub_number):\n",
        "    if (pub_number == 'sn85066408'):\n",
        "        return 'L\\'Italia'\n",
        "    elif (pub_number == '2012271201'):\n",
        "        return 'Cronaca Sovversiva'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6VSQQ3S59Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sources['publication_name'] = sources['publication'].apply(lambda x: get_pub_name(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzukME_p59Fh",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2. Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUqfyF2C59Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "from datetime import timedelta, date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHDI-4nE59Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start from 06.06.1903 and finish 01.05.1919"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITJpslt959Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_ref_1 = date(1903,6,6)\n",
        "date_ref_2 = date(1919,5,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lQ3vbA859Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_by_date(date_item,date_ref_1,date_ref_2):\n",
        "    year = re.findall(r'(\\d{4})-\\d{2}-\\d{2}',date_item)[0]\n",
        "    month = re.findall(r'\\d{4}-(\\d{2})-\\d{2}',date_item)[0]\n",
        "    day = re.findall(r'\\d{4}-\\d{2}-(\\d{2})',date_item)[0]\n",
        "    file_date = date(int(year),int(month),int(day))\n",
        "    if (date_ref_1 <= file_date <= date_ref_2):\n",
        "        return 'included'\n",
        "    else:\n",
        "        return 'not included'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnK5Ag9v59Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sources['status'] = sources['date'].apply(lambda x: filter_by_date(x,date_ref_1,date_ref_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlIciGti59Fp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "aaf3386b-a2c2-43f9-99b6-cd54d7f21780"
      },
      "source": [
        "sources"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>file_content</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>publication</th>\n",
              "      <th>publication_name</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012271201_1904-03-26_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>SOVVERSIVA' .MA Ebdomadario anarchico propaga...</td>\n",
              "      <td>1904-03-26</td>\n",
              "      <td>1904</td>\n",
              "      <td>03</td>\n",
              "      <td>26</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012271201_1912-06-15_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>ANNO Per che per chi mandano morire PREZUSE C...</td>\n",
              "      <td>1912-06-15</td>\n",
              "      <td>1912</td>\n",
              "      <td>06</td>\n",
              "      <td>15</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012271201_1905-04-15_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>ANNO III BARRE VERMONT, SABATO APRILE Num. Sw...</td>\n",
              "      <td>1905-04-15</td>\n",
              "      <td>1905</td>\n",
              "      <td>04</td>\n",
              "      <td>15</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012271201_1905-01-07_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>ANNO NUMERO SATUIIDAY, JANUARY BAREE, VERMONT...</td>\n",
              "      <td>1905-01-07</td>\n",
              "      <td>1905</td>\n",
              "      <td>01</td>\n",
              "      <td>07</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012271201_1915-09-11_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>VIVA fra guerrafondai, della gente seria, che...</td>\n",
              "      <td>1915-09-11</td>\n",
              "      <td>1915</td>\n",
              "      <td>09</td>\n",
              "      <td>11</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>2012271201_1904-08-13_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>ANNO (UNION NUMERO GRON SOVVERSIVA Ebdomadari...</td>\n",
              "      <td>1904-08-13</td>\n",
              "      <td>1904</td>\n",
              "      <td>08</td>\n",
              "      <td>13</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>2012271201_1911-12-16_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>XNNO Poictò tdto noi! 'tf'Z' minatori general...</td>\n",
              "      <td>1911-12-16</td>\n",
              "      <td>1911</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>2012271201_1913-02-08_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>ANNO Aldamas condannato processo iniziatosi G...</td>\n",
              "      <td>1913-02-08</td>\n",
              "      <td>1913</td>\n",
              "      <td>02</td>\n",
              "      <td>08</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>2012271201_1917-01-06_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>Mif f'f WÈÈmm:Amy iWli Li;' moi iONfrmiv SM-:...</td>\n",
              "      <td>1917-01-06</td>\n",
              "      <td>1917</td>\n",
              "      <td>01</td>\n",
              "      <td>06</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>2012271201_1903-10-17_ed-1_seq-1_ocr.txt</td>\n",
              "      <td>NUMERO Ebdomadario anarchico propaganda rivol...</td>\n",
              "      <td>1903-10-17</td>\n",
              "      <td>1903</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>2012271201</td>\n",
              "      <td>Cronaca Sovversiva</td>\n",
              "      <td>included</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>771 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    file_name  ...    status\n",
              "0    2012271201_1904-03-26_ed-1_seq-1_ocr.txt  ...  included\n",
              "1    2012271201_1912-06-15_ed-1_seq-1_ocr.txt  ...  included\n",
              "2    2012271201_1905-04-15_ed-1_seq-1_ocr.txt  ...  included\n",
              "3    2012271201_1905-01-07_ed-1_seq-1_ocr.txt  ...  included\n",
              "4    2012271201_1915-09-11_ed-1_seq-1_ocr.txt  ...  included\n",
              "..                                        ...  ...       ...\n",
              "766  2012271201_1904-08-13_ed-1_seq-1_ocr.txt  ...  included\n",
              "767  2012271201_1911-12-16_ed-1_seq-1_ocr.txt  ...  included\n",
              "768  2012271201_1913-02-08_ed-1_seq-1_ocr.txt  ...  included\n",
              "769  2012271201_1917-01-06_ed-1_seq-1_ocr.txt  ...  included\n",
              "770  2012271201_1903-10-17_ed-1_seq-1_ocr.txt  ...  included\n",
              "\n",
              "[771 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AbjD_hb9SQw",
        "colab": {}
      },
      "source": [
        "# variable containing the documents separately\n",
        "corpus_df = sources[sources['status'] == 'included'].copy().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ybCetXHi9m-y"
      },
      "source": [
        "## 1.2 Removing stop words, punctuation, short words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjphk8Sk59Fu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3bb321aa-b1c7-4ea0-80d8-7e810bd6b34c"
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6e-8sf7RayKz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d4dcb6b2-1ba1-4a06-adfc-4f377f0d1d4e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1MbU3_seOtJ",
        "colab": {}
      },
      "source": [
        "# add tokenized documents in dataframe\n",
        "corpus_df['tokens'] = corpus_df['file_content'].apply(lambda x: nltk.word_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vh3ikvdKfQ60",
        "colab": {}
      },
      "source": [
        "# possible user options:\n",
        "# .isalnum() to removes tokens that include numbers\n",
        "# .isalpha() to remove all tokens that contain more than letters (punctuation and numbers)\n",
        "# .isdecimal() to remove tokens that contain only decimals\n",
        "# .isdigit() to remove tokens that contain only digits\n",
        "\n",
        "# add new column in df with processed tokens (here: keeping only alpha tokens longer than 3 characters + lowercasing)\n",
        "corpus_df['doc_prep'] = corpus_df['tokens'].apply(lambda x: [w.lower() for w in x if (w.isalpha() and len(w) > 2 )])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We2Ph83Y59F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# link to custom stop words: https://drive.google.com/file/d/1VVfW6AKPbb7_fICOG73lEgkXmmZ6BkpC/view?usp=sharing\n",
        "# Upload stop words list into Colab files before proceeding with the next cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J-l00vPyShTM",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "ital_stopwords = stopwords.words('italian')\n",
        "en_stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74eVFiitFFnO",
        "colab": {}
      },
      "source": [
        "stop_words = pd.read_csv('stop_words.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uw-or9DzIVI2",
        "colab": {}
      },
      "source": [
        "stopwords = stop_words['stopword'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLpy-SOmhOmr",
        "colab": {}
      },
      "source": [
        "# add english stop words list to custom stopwords \n",
        "stopwords.extend(en_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3IrR5gbba5ho",
        "colab": {}
      },
      "source": [
        "# to append list of words added by user: ital_stopwords.extend(user_input)\n",
        "# to remove words: ital_stopwords.remove(user_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5SBUnxlpCxEa",
        "colab": {}
      },
      "source": [
        "# add column with tokenized documents without sw\n",
        "corpus_df['doc_prep_nostop'] = corpus_df['doc_prep'].apply(lambda x: [w for w in x if not w in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6S_6yx1UmIbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e16e8a9f-963f-4a5c-ed4f-95d784900405"
      },
      "source": [
        "corpus_df['doc_prep_nostop']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [sovversiva, ebdomadario, anarchico, propagand...\n",
              "1      [mandano, morire, prezuse, contekioni, felice,...\n",
              "2      [barre, vermont, sabato, aprile, num, swhvìv, ...\n",
              "3      [numero, satuiiday, january, baree, vermont, s...\n",
              "4      [viva, guerrafondai, seria, persuade, tirtei, ...\n",
              "                             ...                        \n",
              "766    [union, numero, gron, sovversiva, ebdomadario,...\n",
              "767    [xnno, poictò, tdto, minatori, campi, antracit...\n",
              "768    [aldamas, condannato, processo, iniziatosi, ge...\n",
              "769    [mif, wèèmm, amy, iwli, moi, ionfrmiv, mjèr, c...\n",
              "770    [numero, ebdomadario, anarchico, propaganda, r...\n",
              "Name: doc_prep_nostop, Length: 771, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZHRCw-TLGPK",
        "colab": {}
      },
      "source": [
        "# set the variable to use for topic modelling (if no further options are used)\n",
        "corpus_model = corpus_df['doc_prep_nostop']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAfTMWcX59GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the data after pre-processing in the output folder for verification of pre-processing steps\n",
        "# corpus_df.to_csv(output_folder + '/corpus_df.csv')\n",
        "corpus_df.to_csv(output_folder + '/corpus_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YHKQAqemcgYz"
      },
      "source": [
        "## 1.4 Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ob8M804chYgh",
        "colab": {}
      },
      "source": [
        "# Lemmatization is available in multiple languages in Spacy and not in NLTK (only English)\n",
        "# With Spacy, lemmatization is available for 10 languages. There's also a multi-language option that\n",
        "# should be tested if additional languages are needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spP6fsv-59GO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "b5c1aac9-9a38-46b9-a4fb-9afb457575c3"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QAPblFUwf3F",
        "colab": {},
        "outputId": "34f879ac-554e-4e21-ec00-2ecbe7d7917e"
      },
      "source": [
        "!python3 -m spacy download it_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting it_core_news_sm==2.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.3.0/it_core_news_sm-2.3.0.tar.gz (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 1.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from it_core_news_sm==2.3.0) (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.18.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (7.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (4.47.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (2.24.0)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (41.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->it_core_news_sm==2.3.0) (1.25.9)\n",
            "Using legacy setup.py install for it-core-news-sm, since package 'wheel' is not installed.\n",
            "Installing collected packages: it-core-news-sm\n",
            "    Running setup.py install for it-core-news-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed it-core-news-sm-2.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('it_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8al8riPXA98n",
        "colab": {}
      },
      "source": [
        "import it_core_news_sm\n",
        "it_nlp = it_core_news_sm.load(disable=['tagger', 'parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZHdI_8tmv3y",
        "colab": {}
      },
      "source": [
        "# lemmatization function\n",
        "def lemmatize(doc):\n",
        "  lemmatized_doc = []\n",
        "  for w in doc:\n",
        "    w_lemma = [token.lemma_ for token in it_nlp(w)]\n",
        "    lemmatized_doc.append(w_lemma[0])\n",
        "  return lemmatized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-mYWKgSk9CW",
        "colab": {}
      },
      "source": [
        "# takes a long time to run\n",
        "# add column with lemmatized tokens - directly from the tokens as preprocessing has already been done\n",
        "corpus_df['doc_lemmatized'] = corpus_df['doc_prep_nostop'].apply(lambda x: lemmatize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tvl0MO_FH5yH",
        "colab": {}
      },
      "source": [
        "# variable with lemmatized tokens\n",
        "lemmatized_corpus = corpus_df['doc_lemmatized']\n",
        "# the lemmatized version is not used in this example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o8TRLnFPK6p4"
      },
      "source": [
        "# 2. Topics with LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdwV8h_b59GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "22690de8-3c72-4a27-f6b1-0f36b37451df"
      },
      "source": [
        "#Gensim installation\n",
        "! pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.20 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fr1F0QNdOxy3",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim import corpora, models\n",
        "from gensim.models.wrappers import LdaMallet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "530LX-OLRDcP"
      },
      "source": [
        "## 2.1 Preliminary steps to run LDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuVJZqPE2gMA",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 Creating the dictionary, optional filtering of extreme values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQCa_QvL2vQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if you use lemmatized version replace \"corpus_model\" by lemmatized_corpus\n",
        "id2word = corpora.Dictionary(corpus_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kyAS5ON2x4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2word.filter_extremes(no_below=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GZlLjmvsQAMy"
      },
      "source": [
        "### 2.1.2 Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UzxS5Q3aQ7mh",
        "colab": {}
      },
      "source": [
        "corpus = [id2word.doc2bow(text) for text in corpus_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qadtU7aTQMqg"
      },
      "source": [
        "## 2.2 LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vneO0sPrgtYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ODvMbxSCQh_",
        "colab": {}
      },
      "source": [
        "# set parameters for running the model below\n",
        "topics_nr = []\n",
        "coherence_values_gensim = []\n",
        "for num_topics in range(3, 20):\n",
        "  # gensim default parameters\n",
        "  # model = gensim.models.ldamulticore.LdaMulticore(corpus, id2word=id2word, num_topics=num_topics)\n",
        "  # preset1\n",
        "  # model = gensim.models.ldamulticore.LdaMulticore(corpus, id2word=id2word, num_topics=num_topics,chunksize=2000, iterations = 400, passes = 20)\n",
        "  # preset2\n",
        "  model = gensim.models.ldamulticore.LdaMulticore(corpus, id2word=id2word, num_topics=num_topics,chunksize=1000, iterations = 200, passes = 10)\n",
        "  topic_print = model.print_topics(num_words=30)\n",
        "  df_topic_print = pd.DataFrame(topic_print, columns=['topic_id','words'])\n",
        "  # change the file name to have a different file for each set of parameters\n",
        "  csv_filenametxt = \"{}/preset_2_{}_topics.csv\"\n",
        "  csv_filename = csv_filenametxt.format(output_folder,num_topics)\n",
        "  df_topic_print.to_csv(csv_filename)\n",
        "  coherencemodel = CoherenceModel(model=model, texts=corpus_model, dictionary=id2word, coherence='c_v')\n",
        "  coherence_value = coherencemodel.get_coherence()\n",
        "  coherence_values_gensim.append(coherence_value)\n",
        "  topics_nr.append(str(num_topics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FLfpXjulCcS4",
        "colab": {}
      },
      "source": [
        "df_coherence = pd.DataFrame(topics_nr, columns=['topic_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnURiwZD59Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_coherence['topic_coherence'] = coherence_values_gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNKiyM0ol8_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name of the file depending on the preset\n",
        "df_coherence.to_csv(output_folder + '/preset_2_coherence_values.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}