{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modelling_mallet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FwlEnNvQJNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AZPes5RSYG2",
        "colab_type": "text"
      },
      "source": [
        "# 0. Preliminary step to get sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRiBwgFTg2p",
        "colab_type": "text"
      },
      "source": [
        "This preliminary step is reproducing Lorella's workflow Python file:\n",
        "https://i-lab.public.data.uu.nl/vault-ocex/ChroniclItaly%20-%20Italian%20American%20newspapers%20corpus%20from%201898%20to%201920%5B1529330521%5D/original/\n",
        "I just added a folder \"data_1\" to keep all files in one folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK81EVJim0Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir 'data1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_c4pYzXsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base URL\n",
        "chronam = 'https://chroniclingamerica.loc.gov/'\n",
        "\n",
        "# Chronicling America search results\n",
        "results = 'https://chroniclingamerica.loc.gov/search/pages/results/?date1=1880&date2=1920&searchType=advanced&language=ita&sequence=1&lccn=2012271201&lccn=sn85066408&lccn=sn85055164&lccn=sn85054967&lccn=sn88064299&lccn=sn84037024&lccn=sn84037025&lccn=sn86092310&proxdistance=5&state=California&state=District+of+Columbia&state=Massachusetts&state=Pennsylvania&state=Piedmont&state=Vermont&state=West+Virginia&rows=100&ortext=&proxtext=&phrasetext=&andtext=&dateFilterType=yearRange&page=11&sort=date'\n",
        "\n",
        "# Count to keep track of downloaded files\n",
        "count = 0\n",
        "\n",
        "# Gets search results in JSON format\n",
        "results_json = results + '&format=json'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwqRYbydrq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns JSON \n",
        "def get_json(url):\n",
        "    data = requests.get(url)\n",
        "    return(json.loads(data.content))\n",
        "    \n",
        "data = get_json(results_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv32dwDndnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cycle through JSON results\n",
        "for page in data['items']:\n",
        "    # Create URL\n",
        "    hit = str(page['id'])\n",
        "    seed = hit + 'ocr.txt'\n",
        "    download_url = chronam + seed\n",
        " \n",
        "    # Create file name\n",
        "    file_name = download_url.replace('/', '_')\n",
        "    file_name = 'data1/' + file_name[41:]\n",
        "    \n",
        "    # Download .txt of the page\n",
        "    urllib.request.urlretrieve(download_url, str(file_name))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUvuIpuzT8ke",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTOrDXSYUIc3",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Grouping all texts files\n",
        "A dataframe is first created to keep individual files at their initial state, and the name of each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u857KfY9WXN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FNWdVJT1v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of the file names\n",
        "files_list = os.listdir('data1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lLD0OpiXv5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#insert file names into a df\n",
        "sources = pd.DataFrame(files_list, columns=['file_name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PgOwPnqCfAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to read the content of the text files\n",
        "def readTxtContent(fileName):\n",
        "  with open('data1/' + fileName, 'r') as file:\n",
        "    return ' ' + file.read().replace('\\n', ' ') + ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEVIKHziQv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding a column to the dataframe containing file content\n",
        "sources['file_content'] = sources['file_name'].apply(lambda x: readTxtContent(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ffdAsOXQSBe",
        "colab_type": "code",
        "outputId": "237d1341-f6ee-4d03-b5a6-39fe1ddee581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for verification purposes later, count the nr of characters for each content\n",
        "sources['file_len'] = sources['file_content'].apply(lambda x: len(x))\n",
        "sources['file_len'].sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwTwsGsED8uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variable containing all texts together\n",
        "corpus = ''\n",
        "for i in range(len(sources)):\n",
        "  corpus += sources['file_content'][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBu87u_SSM-9",
        "colab_type": "code",
        "outputId": "4547fdc1-44ea-4373-905a-0bf47111a43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check length\n",
        "len(corpus)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybCetXHi9m-y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.2 Removing stop words, punctuation, short words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-8sf7RayKz",
        "colab_type": "code",
        "outputId": "f9f16cbd-422c-423e-858b-e93fce596b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZjtQSrV_lX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuation and lower case (then depending on user input, leave the possibility to do one or the other)\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "tokenized_corpus = [w.lower() for w in tokens if w.isalnum()]\n",
        "# lower case, remove punctuation and only keep words that have more than 3 letters\n",
        "tokenized_corpus = [w.lower() for w in tokens if (w.isalnum() and len(w) > 3 )]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IrR5gbba5ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show list of default Italian stopwords\n",
        "# stopwords.words('italian')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo3hn7mWbCtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add custom stop word\n",
        "ital_stopwords = stopwords.words('italian')\n",
        "# to append list of words added by user: ital_stopwords.extend(user_input)\n",
        "# to remove words: ital_stopwords.remove(user_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qw4i2O_CTdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.it.stop_words import STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SBUnxlpCxEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_it_sw = STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXh4tcgbGg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"stopwords.words('italian')\" can be replaced by a custom list input by the user\n",
        "tokenized_corpus_without_sw = [w for w in tokenized_corpus if not w in spacy_it_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKs7aXWLbVWo",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyl0C-xkbLiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euUXP7UHbaN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize with needed language\n",
        "stemmer = SnowballStemmer(\"italian\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8fTjp5bean",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_corpus = [stemmer.stem(w) for w in tokenized_corpus_without_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKQAqemcgYz",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8M804chYgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize is available in multiple languages in Spacy and not in NLTK (only English)\n",
        "# With Spacy, lemmatization is available for 10 languages. There's also a multi-language option that\n",
        "# should be tested if additional languages are needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QAPblFUwf3F",
        "colab_type": "code",
        "outputId": "6eb3b9ce-516a-42b1-f3ba-17f2bae61d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "\n",
        "!python -m spacy download it_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting it_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.2.5/it_core_news_sm-2.2.5.tar.gz (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 674kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from it_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.18.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: it-core-news-sm\n",
            "  Building wheel for it-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for it-core-news-sm: filename=it_core_news_sm-2.2.5-cp36-none-any.whl size=14471130 sha256=2573639ffcae6abd5f2952a66713f35eaea2b6651b9a2b4c05266cd0b7037719\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kpha7u0/wheels/a1/01/c2/127ab92cc5e3c7f36b5cd4bff28d1c29c313962a2ba913e720\n",
            "Successfully built it-core-news-sm\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('it_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8al8riPXA98n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import it_core_news_sm\n",
        "it_nlp = it_core_news_sm.load(disable=['tagger', 'parser', 'ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JewnrBc6cmV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe with corpus\n",
        "corpus_df = pd.DataFrame(tokenized_corpus_without_sw, columns=['tokens'])\n",
        "# to test only on 30 lines, use: corpus_df = pd.DataFrame(tokenized_corpus_without_sw[0:30], columns=['tokens'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYwtxl3XiMFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add column with lemmatized version\n",
        "corpus_df['lemmatized_token'] = corpus_df['tokens'].apply(lambda x: [token.lemma_ for token in it_nlp(x)] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8TRLnFPK6p4",
        "colab_type": "text"
      },
      "source": [
        "# 2. Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0NrktL1el7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installation setup that works for Mallet: https://github.com/polsci/colab-gensim-mallet/blob/master/topic-modeling-with-colab-gensim-mallet.ipynb\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH7oPbxGe46O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD1MaCjRfKOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr1F0QNdOxy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim import corpora, models\n",
        "from gensim.models.wrappers import LdaMallet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzxS5Q3aQ7mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [d.split() for d in tokenized_corpus_without_sw]\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(dataset)\n",
        "\n",
        "# Create Corpus\n",
        "texts = dataset\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNMEMY3LaQQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ecaf2fe1-93b7-4601-b50d-2f545aecb58e"
      },
      "source": [
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7VpXA0cahdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "64950090-dac5-4dd7-d5da-2537e63b1668"
      },
      "source": [
        "ldamallet.print_topics()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.026*\"york\" + 0.017*\"giornale\" + 0.017*\"società\" + 0.014*\"francia\" + 0.013*\"viaggio\" + 0.011*\"capo\" + 0.011*\"colla\" + 0.011*\"parole\" + 0.010*\"strada\" + 0.008*\"altra\"'),\n",
              " (1,\n",
              "  '0.058*\"italiana\" + 0.027*\"mente\" + 0.011*\"sera\" + 0.010*\"paesi\" + 0.008*\"edoardo\" + 0.008*\"perchè\" + 0.008*\"vuol\" + 0.008*\"ministero\" + 0.007*\"membri\" + 0.007*\"marina\"'),\n",
              " (2,\n",
              "  '0.020*\"parigi\" + 0.019*\"mano\" + 0.013*\"francesi\" + 0.013*\"cose\" + 0.012*\"disse\" + 0.012*\"chiesa\" + 0.011*\"regina\" + 0.010*\"roosevelt\" + 0.010*\"presso\" + 0.010*\"politica\"'),\n",
              " (3,\n",
              "  '0.027*\"patria\" + 0.015*\"patrizi\" + 0.014*\"presidente\" + 0.013*\"giustizia\" + 0.012*\"amici\" + 0.011*\"mento\" + 0.010*\"circulation\" + 0.010*\"venezia\" + 0.009*\"montgomery\" + 0.008*\"operai\"'),\n",
              " (4,\n",
              "  '0.057*\"roma\" + 0.011*\"delia\" + 0.011*\"leone\" + 0.008*\"lire\" + 0.008*\"londra\" + 0.008*\"socialisti\" + 0.007*\"viene\" + 0.006*\"inglese\" + 0.006*\"inglesi\" + 0.006*\"porti\"'),\n",
              " (5,\n",
              "  '0.056*\"italia\" + 0.021*\"quotidiano\" + 0.015*\"genova\" + 0.010*\"milano\" + 0.010*\"zione\" + 0.009*\"situazione\" + 0.008*\"viva\" + 0.007*\"nazioni\" + 0.007*\"deposito\" + 0.007*\"libertà\"'),\n",
              " (6,\n",
              "  '0.034*\"italia\" + 0.020*\"cardinali\" + 0.018*\"popolo\" + 0.014*\"vive\" + 0.012*\"dato\" + 0.010*\"legge\" + 0.009*\"news\" + 0.009*\"xvii\" + 0.008*\"standard\" + 0.007*\"ferri\"'),\n",
              " (7,\n",
              "  '0.031*\"cardinale\" + 0.018*\"uniti\" + 0.016*\"vaticano\" + 0.012*\"proposito\" + 0.012*\"pubblica\" + 0.011*\"pacifico\" + 0.009*\"possono\" + 0.009*\"zione\" + 0.009*\"corpo\" + 0.008*\"case\"'),\n",
              " (8,\n",
              "  '0.039*\"italiano\" + 0.013*\"quotidiano\" + 0.013*\"polizia\" + 0.013*\"americane\" + 0.012*\"essa\" + 0.011*\"umanità\" + 0.011*\"macchine\" + 0.009*\"americani\" + 0.009*\"vero\" + 0.008*\"notizia\"'),\n",
              " (9,\n",
              "  '0.022*\"marconi\" + 0.021*\"morte\" + 0.018*\"luglio\" + 0.016*\"altre\" + 0.013*\"guglielmo\" + 0.012*\"coast\" + 0.012*\"pochi\" + 0.012*\"voti\" + 0.012*\"giornali\" + 0.011*\"suol\"'),\n",
              " (10,\n",
              "  '0.030*\"nome\" + 0.018*\"grandi\" + 0.016*\"camera\" + 0.015*\"francisco\" + 0.012*\"cioè\" + 0.010*\"market\" + 0.010*\"lavori\" + 0.009*\"parenti\" + 0.008*\"collegio\" + 0.008*\"ordine\"'),\n",
              " (11,\n",
              "  '0.042*\"daily\" + 0.021*\"maggio\" + 0.018*\"russia\" + 0.014*\"united\" + 0.014*\"guerra\" + 0.012*\"california\" + 0.010*\"francese\" + 0.009*\"vittime\" + 0.009*\"reca\" + 0.008*\"presidente\"'),\n",
              " (12,\n",
              "  '0.022*\"giugno\" + 0.017*\"xiii\" + 0.014*\"ufficiali\" + 0.011*\"parigi\" + 0.009*\"onore\" + 0.007*\"fede\" + 0.007*\"nuove\" + 0.006*\"pare\" + 0.006*\"trono\" + 0.006*\"pace\"'),\n",
              " (13,\n",
              "  '0.037*\"papa\" + 0.014*\"piroscafi\" + 0.010*\"punto\" + 0.010*\"forte\" + 0.008*\"telegramma\" + 0.008*\"mese\" + 0.008*\"nazione\" + 0.008*\"segretario\" + 0.008*\"paper\" + 0.007*\"operai\"'),\n",
              " (14,\n",
              "  '0.034*\"italiani\" + 0.026*\"italiano\" + 0.025*\"largest\" + 0.024*\"states\" + 0.015*\"mattino\" + 0.011*\"western\" + 0.009*\"prova\" + 0.009*\"fatta\" + 0.009*\"venire\" + 0.008*\"ricevuto\"'),\n",
              " (15,\n",
              "  '0.035*\"dispaccio\" + 0.032*\"numero\" + 0.018*\"pontefice\" + 0.016*\"perchè\" + 0.012*\"autorità\" + 0.010*\"costa\" + 0.009*\"zioni\" + 0.009*\"rapida\" + 0.009*\"nuove\" + 0.008*\"commissione\"'),\n",
              " (16,\n",
              "  '0.027*\"roma\" + 0.018*\"lotta\" + 0.018*\"napoli\" + 0.016*\"condizioni\" + 0.013*\"conclave\" + 0.012*\"navigazione\" + 0.012*\"partito\" + 0.011*\"principe\" + 0.009*\"washington\" + 0.009*\"czar\"'),\n",
              " (17,\n",
              "  '0.048*\"telegrafico\" + 0.021*\"visita\" + 0.019*\"daily\" + 0.018*\"italiane\" + 0.017*\"fatti\" + 0.013*\"costa\" + 0.013*\"causa\" + 0.008*\"nuova\" + 0.007*\"cuore\" + 0.006*\"azione\"'),\n",
              " (18,\n",
              "  '0.029*\"italian\" + 0.023*\"giornale\" + 0.021*\"telegrafico\" + 0.014*\"notizie\" + 0.013*\"truppe\" + 0.011*\"specialmente\" + 0.009*\"gravi\" + 0.008*\"stampa\" + 0.007*\"lavoratori\" + 0.007*\"austria\"'),\n",
              " (19,\n",
              "  '0.044*\"dispaccio\" + 0.038*\"gran\" + 0.021*\"america\" + 0.015*\"pietro\" + 0.013*\"ebrei\" + 0.012*\"folla\" + 0.011*\"vittorio\" + 0.010*\"capitale\" + 0.009*\"dollari\" + 0.008*\"aprile\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    }
  ]
}