{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of topic_modelling_spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FwlEnNvQJNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AZPes5RSYG2",
        "colab_type": "text"
      },
      "source": [
        "# 0. Preliminary step to get sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRiBwgFTg2p",
        "colab_type": "text"
      },
      "source": [
        "This preliminary step is reproducing Lorella's workflow Python file:\n",
        "https://i-lab.public.data.uu.nl/vault-ocex/ChroniclItaly%20-%20Italian%20American%20newspapers%20corpus%20from%201898%20to%201920%5B1529330521%5D/original/\n",
        "I just added a folder \"data_1\" to keep all files in one folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK81EVJim0Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir 'data1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_c4pYzXsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base URL\n",
        "chronam = 'https://chroniclingamerica.loc.gov/'\n",
        "\n",
        "# Chronicling America search results\n",
        "results = 'https://chroniclingamerica.loc.gov/search/pages/results/?date1=1880&date2=1920&searchType=advanced&language=ita&sequence=1&lccn=2012271201&lccn=sn85066408&lccn=sn85055164&lccn=sn85054967&lccn=sn88064299&lccn=sn84037024&lccn=sn84037025&lccn=sn86092310&proxdistance=5&state=California&state=District+of+Columbia&state=Massachusetts&state=Pennsylvania&state=Piedmont&state=Vermont&state=West+Virginia&rows=100&ortext=&proxtext=&phrasetext=&andtext=&dateFilterType=yearRange&page=11&sort=date'\n",
        "\n",
        "# Count to keep track of downloaded files\n",
        "count = 0\n",
        "\n",
        "# Gets search results in JSON format\n",
        "results_json = results + '&format=json'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwqRYbydrq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns JSON \n",
        "def get_json(url):\n",
        "    data = requests.get(url)\n",
        "    return(json.loads(data.content))\n",
        "    \n",
        "data = get_json(results_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv32dwDndnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cycle through JSON results\n",
        "for page in data['items']:\n",
        "    # Create URL\n",
        "    hit = str(page['id'])\n",
        "    seed = hit + 'ocr.txt'\n",
        "    download_url = chronam + seed\n",
        " \n",
        "    # Create file name\n",
        "    file_name = download_url.replace('/', '_')\n",
        "    file_name = 'data1/' + file_name[41:]\n",
        "    \n",
        "    # Download .txt of the page\n",
        "    urllib.request.urlretrieve(download_url, str(file_name))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUvuIpuzT8ke",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTOrDXSYUIc3",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Grouping all texts files\n",
        "A dataframe is first created to keep individual files at their initial state, and the name of each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u857KfY9WXN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FNWdVJT1v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of the file names\n",
        "files_list = os.listdir('data1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lLD0OpiXv5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#insert file names into a df\n",
        "sources = pd.DataFrame(files_list, columns=['file_name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PgOwPnqCfAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to read the content of the text files\n",
        "def readTxtContent(fileName):\n",
        "  with open('data1/' + fileName, 'r') as file:\n",
        "    return ' ' + file.read().replace('\\n', ' ') + ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEVIKHziQv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding a column to the dataframe containing file content\n",
        "sources['file_content'] = sources['file_name'].apply(lambda x: readTxtContent(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ffdAsOXQSBe",
        "colab_type": "code",
        "outputId": "3fe06a18-1819-4757-9cba-8ad9afa3773c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for verification purposes later, count the nr of characters for each content\n",
        "sources['file_len'] = sources['file_content'].apply(lambda x: len(x))\n",
        "sources['file_len'].sum()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwTwsGsED8uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variable containing all texts together\n",
        "corpus = ''\n",
        "for i in range(len(sources)):\n",
        "  corpus += sources['file_content'][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBu87u_SSM-9",
        "colab_type": "code",
        "outputId": "e4abac32-d036-43b5-d6f0-2ac053f0d81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check length\n",
        "len(corpus)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybCetXHi9m-y",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Pre-processing options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEmwoDjMPmxa",
        "colab_type": "text"
      },
      "source": [
        "Options for the user to work on lower cased version, exclude short words, remove punctuation, remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZjtQSrV_lX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hykjXuVyh9iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding punctuation rules (splitting punctuation that is before or after a word with no whitespace) as\n",
        "# it is not included by default within the tokenizer\n",
        "\n",
        "custom_nlp = Italian() # language\n",
        "\n",
        "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
        "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
        "\n",
        "def customize_tokenizer(nlp):\n",
        "    # Adds support to use `-` as the delimiter for tokenization\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
        "                     suffix_search=suffix_re.search,\n",
        "                     token_match=None\n",
        "                     )\n",
        "\n",
        "\n",
        "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HWCOcpeq4Sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not using directly the \"nlp(corpus)\" from spacy as the length of the \n",
        "# corpus variable exceeds the 1000000 length limitation from spacy. I'm using a workaround with \"tokenizer()\"\n",
        "# and it's not really good because the tokenization results are not satisfactory with this. It makes pre-processing\n",
        "# more complex than with NLTK and losing some of Spacy's functionalities. \n",
        "# this opens a question about switching back to NLTK for the pre processing steps until tokenization\n",
        "# and only use some of Spacy functionalities (for ex. lemmatization). it depends on how much we want to use Spacy\n",
        "# later in the process. If the modelling/categorization parts only happen with Gensim, then I think it's ok to switch back\n",
        "# to NLTK for some pre-processing steps.\n",
        "# Another option would be to divide the corpus in smaller chunks, but I don't know how this impacts (or not) further\n",
        "# steps if Spacy is used at a later stage (would the models be accurate?). Then, if for the more advanced steps\n",
        "# Gensim is used exclusively and not Spacy, I think that NLTK could mostly be used for pre-processing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGG44Mjcvle_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = custom_nlp.tokenizer(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdwlyx2esFpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uncomment to check results\n",
        "# ! I see that Spacy defaults prefix/suffix removers don't handle the cases of capital letters + punctuation without whitespaces\n",
        "# for token in tokens[0:70]:\n",
        "#    print(token.text, token.lemma_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeflceKM_7OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lower case tokens\n",
        "tokens_low = [token.lower_ for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txWZd_OrIZ53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens excluding punctuation, white spaces, and words smaller than 3 letters\n",
        "tokens_punct_size = [token.orth_ for token in tokens if not token.is_punct | token.is_space | len(token.text) < 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdeaRM_DQrJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment last line of this cell to show list of default Italian stopwords\n",
        "from spacy.lang.it.stop_words import STOP_WORDS\n",
        "# STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnC1H7HbI-tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining all of the above and excluding default stop words list\n",
        "tokens_nostop = [token.lower_ for token in tokens if not (token.is_stop or token.is_punct or token.is_space or len(token.text) < 4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW0gKNlhPOWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to add/remove stop words depending on user input\n",
        "# nlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",}\n",
        "# nlp.Defaults.stop_words -= {\"whatever\", \"whenever\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odyz9sWNTraN",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Stem "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tegjxs6apC6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemming is available via NLTK and not Spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fDKhXmBUD0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhg56o_1LeA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize with needed language\n",
        "stemmer = SnowballStemmer(\"italian\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDC3mIa3WzFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_corpus = [stemmer.stem(w) for w in tokens_nostop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4prU53t7pV90",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUmkTpHhWvHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens_lemma = [token.lemma_ for token in tokens if not (token.is_stop or token.is_punct or token.is_space or len(token.text) < 4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8TRLnFPK6p4",
        "colab_type": "text"
      },
      "source": [
        "# 2. Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PjHRUzLDbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kibSiMo0PxBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [d.split() for d in tokens_nostop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnwCJGqHPrTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbMCT1sRmY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqlYyvDZSer6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import models\n",
        "\n",
        "tfidf = models.TfidfModel(corpus) #basic gensim model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ZdxEeOTf6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_tfidf = tfidf[corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo9066rMU8KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize transformation with 5 topics (need to find how to calculate the optimum number of topics with gensim)\n",
        "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)  \n",
        "corpus_lsi = lsi_model[corpus_tfidf] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibc6JKYfVldR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "e6c074a1-e7a1-4ad8-9e39-9d35bf81baeb"
      },
      "source": [
        "# see results\n",
        "# ! There seems to be words such as \"daily\" \"with\"...\n",
        "lsi_model.print_topics()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '1.000*\"roma\" + -0.002*\"dispaccio\" + -0.001*\"united\" + -0.001*\"camera\" + -0.001*\"napoli\" + -0.001*\"condizioni\" + 0.001*\"genova\" + 0.001*\"western\" + 0.001*\"giustizia\" + -0.001*\"terni\"'),\n",
              " (1,\n",
              "  '-0.960*\"dispaccio\" + -0.277*\"daily\" + 0.038*\"italiano\" + 0.027*\"italiani\" + -0.006*\"telegrafico\" + 0.005*\"cardinale\" + -0.002*\"roma\" + 0.001*\"politica\" + -0.001*\"piroscafi\" + 0.001*\"with\"'),\n",
              " (2,\n",
              "  '0.995*\"italiano\" + -0.071*\"daily\" + 0.060*\"dispaccio\" + -0.010*\"cardinale\" + 0.010*\"telegrafico\" + 0.004*\"italiana\" + 0.002*\"popolo\" + 0.002*\"società\" + 0.001*\"napoli\" + -0.001*\"italiani\"'),\n",
              " (3,\n",
              "  '-0.883*\"telegrafico\" + 0.348*\"cardinale\" + 0.275*\"daily\" + 0.129*\"papa\" + -0.070*\"dispaccio\" + 0.036*\"italiano\" + -0.005*\"italiani\" + 0.003*\"italiana\" + 0.002*\"difesa\" + -0.002*\"dare\"'),\n",
              " (4,\n",
              "  '0.990*\"papa\" + 0.100*\"telegrafico\" + -0.088*\"cardinale\" + -0.028*\"daily\" + 0.007*\"dispaccio\" + -0.003*\"specialmente\" + -0.003*\"italiano\" + -0.003*\"amici\" + 0.003*\"sindaco\" + 0.003*\"vive\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    }
  ]
}