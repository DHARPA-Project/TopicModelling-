{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FwlEnNvQJNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AZPes5RSYG2",
        "colab_type": "text"
      },
      "source": [
        "# 0. Preliminary step to get sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRiBwgFTg2p",
        "colab_type": "text"
      },
      "source": [
        "This preliminary step is reproducing Lorella's workflow Python file:\n",
        "https://i-lab.public.data.uu.nl/vault-ocex/ChroniclItaly%20-%20Italian%20American%20newspapers%20corpus%20from%201898%20to%201920%5B1529330521%5D/original/\n",
        "I just added a folder \"data_1\" to keep all files in one folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK81EVJim0Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir 'data1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_c4pYzXsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base URL\n",
        "chronam = 'https://chroniclingamerica.loc.gov/'\n",
        "\n",
        "# Chronicling America search results\n",
        "results = 'https://chroniclingamerica.loc.gov/search/pages/results/?date1=1880&date2=1920&searchType=advanced&language=ita&sequence=1&lccn=2012271201&lccn=sn85066408&lccn=sn85055164&lccn=sn85054967&lccn=sn88064299&lccn=sn84037024&lccn=sn84037025&lccn=sn86092310&proxdistance=5&state=California&state=District+of+Columbia&state=Massachusetts&state=Pennsylvania&state=Piedmont&state=Vermont&state=West+Virginia&rows=100&ortext=&proxtext=&phrasetext=&andtext=&dateFilterType=yearRange&page=11&sort=date'\n",
        "\n",
        "# Count to keep track of downloaded files\n",
        "count = 0\n",
        "\n",
        "# Gets search results in JSON format\n",
        "results_json = results + '&format=json'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwqRYbydrq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns JSON \n",
        "def get_json(url):\n",
        "    data = requests.get(url)\n",
        "    return(json.loads(data.content))\n",
        "    \n",
        "data = get_json(results_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv32dwDndnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cycle through JSON results\n",
        "for page in data['items']:\n",
        "    # Create URL\n",
        "    hit = str(page['id'])\n",
        "    seed = hit + 'ocr.txt'\n",
        "    download_url = chronam + seed\n",
        " \n",
        "    # Create file name\n",
        "    file_name = download_url.replace('/', '_')\n",
        "    file_name = 'data1/' + file_name[41:]\n",
        "    \n",
        "    # Download .txt of the page\n",
        "    urllib.request.urlretrieve(download_url, str(file_name))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUvuIpuzT8ke",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTOrDXSYUIc3",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Grouping all texts files\n",
        "A dataframe is first created to keep individual files at their initial state, and the name of each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u857KfY9WXN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FNWdVJT1v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of the file names\n",
        "files_list = os.listdir('data1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lLD0OpiXv5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#insert file names into a df\n",
        "sources = pd.DataFrame(files_list, columns=['file_name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PgOwPnqCfAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to read the content of the text files\n",
        "def readTxtContent(fileName):\n",
        "  with open('data1/' + fileName, 'r') as file:\n",
        "    return ' ' + file.read().replace('\\n', ' ') + ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEVIKHziQv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding a column to the dataframe containing file content\n",
        "sources['file_content'] = sources['file_name'].apply(lambda x: readTxtContent(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ffdAsOXQSBe",
        "colab_type": "code",
        "outputId": "237d1341-f6ee-4d03-b5a6-39fe1ddee581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for verification purposes later, count the nr of characters for each content\n",
        "sources['file_len'] = sources['file_content'].apply(lambda x: len(x))\n",
        "sources['file_len'].sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwTwsGsED8uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variable containing all texts together\n",
        "corpus = ''\n",
        "for i in range(len(sources)):\n",
        "  corpus += sources['file_content'][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBu87u_SSM-9",
        "colab_type": "code",
        "outputId": "4547fdc1-44ea-4373-905a-0bf47111a43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check length\n",
        "len(corpus)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybCetXHi9m-y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.2 Removing stop words, punctuation, short words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-8sf7RayKz",
        "colab_type": "code",
        "outputId": "f9f16cbd-422c-423e-858b-e93fce596b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZjtQSrV_lX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuation and lower case (then depending on user input, leave the possibility to do one or the other)\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "tokenized_corpus = [w.lower() for w in tokens if w.isalnum()]\n",
        "# lower case, remove punctuation and only keep words that have more than 3 letters\n",
        "tokenized_corpus = [w.lower() for w in tokens if (w.isalnum() and len(w) > 3 )]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IrR5gbba5ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show list of default Italian stopwords\n",
        "# stopwords.words('italian')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo3hn7mWbCtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add custom stop word\n",
        "ital_stopwords = stopwords.words('italian')\n",
        "# to append list of words added by user: ital_stopwords.extend(user_input)\n",
        "# to remove words: ital_stopwords.remove(user_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qw4i2O_CTdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.it.stop_words import STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SBUnxlpCxEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_it_sw = STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXh4tcgbGg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"stopwords.words('italian')\" can be replaced by a custom list input by the user\n",
        "tokenized_corpus_without_sw = [w for w in tokenized_corpus if not w in spacy_it_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKs7aXWLbVWo",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyl0C-xkbLiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euUXP7UHbaN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize with needed language\n",
        "stemmer = SnowballStemmer(\"italian\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8fTjp5bean",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_corpus = [stemmer.stem(w) for w in tokenized_corpus_without_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKQAqemcgYz",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8M804chYgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatize is available in multiple languages in Spacy and not in NLTK (only English)\n",
        "# With Spacy, lemmatization is available for 10 languages. There's also a multi-language option that\n",
        "# should be tested if additional languages are needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QAPblFUwf3F",
        "colab_type": "code",
        "outputId": "6eb3b9ce-516a-42b1-f3ba-17f2bae61d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "\n",
        "!python -m spacy download it_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting it_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.2.5/it_core_news_sm-2.2.5.tar.gz (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 674kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from it_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.18.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: it-core-news-sm\n",
            "  Building wheel for it-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for it-core-news-sm: filename=it_core_news_sm-2.2.5-cp36-none-any.whl size=14471130 sha256=2573639ffcae6abd5f2952a66713f35eaea2b6651b9a2b4c05266cd0b7037719\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kpha7u0/wheels/a1/01/c2/127ab92cc5e3c7f36b5cd4bff28d1c29c313962a2ba913e720\n",
            "Successfully built it-core-news-sm\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('it_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8al8riPXA98n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import it_core_news_sm\n",
        "it_nlp = it_core_news_sm.load(disable=['tagger', 'parser', 'ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JewnrBc6cmV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe with corpus\n",
        "corpus_df = pd.DataFrame(tokenized_corpus_without_sw, columns=['tokens'])\n",
        "# to test only on 30 lines, use: corpus_df = pd.DataFrame(tokenized_corpus_without_sw[0:30], columns=['tokens'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYwtxl3XiMFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add column with lemmatized version\n",
        "corpus_df['lemmatized_token'] = corpus_df['tokens'].apply(lambda x: [token.lemma_ for token in it_nlp(x)] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8TRLnFPK6p4",
        "colab_type": "text"
      },
      "source": [
        "# 2. Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PjHRUzLDbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kibSiMo0PxBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [d.split() for d in tokenized_corpus_without_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnwCJGqHPrTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbMCT1sRmY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# doc2bow: converts document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples\n",
        "corpus = [dictionary.doc2bow(text) for text in dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRroQv74MUl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optional\n",
        "# TFIDF for better performance\n",
        "# https://en.wikipedia.org/wiki/Tf–idf / \n",
        "# https://rare-technologies.com/pivoted-document-length-normalisation/\n",
        "\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqlYyvDZSer6",
        "colab_type": "code",
        "outputId": "58ffe7f3-8294-4f16-acb6-a3557bc729e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# LDA Model with TFIDF\n",
        "# the number of topics is set below in \"num_topics\"\n",
        "model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
        "model.print_topics()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.057*\"durò\" + 0.025*\"leone\" + 0.010*\"anzi\" + 0.010*\"pontificato\" + 0.010*\"sommo\" + 0.010*\"secoli\" + 0.010*\"pagina\" + 0.009*\"bacchino\" + 0.009*\"italiana\" + 0.008*\"papa\"'),\n",
              " (1,\n",
              "  '0.011*\"telegrafico\" + 0.011*\"breve\" + 0.011*\"eletto\" + 0.010*\"usci\" + 0.010*\"colonna\" + 0.010*\"xlll\" + 0.010*\"vili\" + 0.010*\"difatti\" + 0.010*\"niezione\" + 0.010*\"1513\"'),\n",
              " (2,\n",
              "  '0.022*\"conclave\" + 0.015*\"italia\" + 0.014*\"roma\" + 0.014*\"dispaccio\" + 0.012*\"piti\" + 0.011*\"italiano\" + 0.011*\"iregorlo\" + 0.010*\"1775\" + 0.010*\"1447\" + 0.008*\"italiani\"'),\n",
              " (3,\n",
              "  '0.012*\"rapida\" + 0.012*\"continua\" + 0.010*\"pupa\" + 0.010*\"1800\" + 0.010*\"daily\" + 0.010*\"1633\" + 0.010*\"elezlo\" + 0.010*\"belloza\" + 0.010*\"1829\" + 0.006*\"york\"'),\n",
              " (4,\n",
              "  '0.021*\"febbraio\" + 0.016*\"numero\" + 0.010*\"pena\" + 0.010*\"1878\" + 0.010*\"pece\" + 0.010*\"nicolò\" + 0.010*\"principiò\" + 0.010*\"clemente\" + 0.005*\"giornali\" + 0.004*\"italian\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EZeejp-83sh",
        "colab_type": "code",
        "outputId": "510ec8ae-0ffe-438a-edf9-5d63e486e138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# LDA Model without TFIDF\n",
        "# the number of topics is set below in \"num_topics\"\n",
        "model = models.LdaModel(corpus, id2word=dictionary, num_topics=5)\n",
        "model.print_topics()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.027*\"leone\" + 0.011*\"colonna\" + 0.010*\"iregorlo\" + 0.010*\"difatti\" + 0.010*\"1800\" + 0.010*\"niezione\" + 0.010*\"1447\" + 0.008*\"giornale\" + 0.006*\"morte\" + 0.005*\"nome\"'),\n",
              " (1,\n",
              "  '0.059*\"durò\" + 0.015*\"numero\" + 0.010*\"italiano\" + 0.010*\"pupa\" + 0.010*\"pagina\" + 0.010*\"nicolò\" + 0.010*\"clemente\" + 0.009*\"elezlo\" + 0.004*\"grandi\" + 0.004*\"largest\"'),\n",
              " (2,\n",
              "  '0.021*\"febbraio\" + 0.015*\"italia\" + 0.012*\"rapida\" + 0.012*\"telegrafico\" + 0.011*\"secoli\" + 0.010*\"1829\" + 0.010*\"1775\" + 0.010*\"bacchino\" + 0.007*\"york\" + 0.005*\"states\"'),\n",
              " (3,\n",
              "  '0.013*\"roma\" + 0.013*\"dispaccio\" + 0.011*\"anzi\" + 0.011*\"breve\" + 0.010*\"pontificato\" + 0.010*\"1878\" + 0.010*\"xlll\" + 0.010*\"sommo\" + 0.010*\"pece\" + 0.010*\"principiò\"'),\n",
              " (4,\n",
              "  '0.021*\"conclave\" + 0.011*\"continua\" + 0.011*\"piti\" + 0.010*\"eletto\" + 0.010*\"usci\" + 0.010*\"pena\" + 0.010*\"vili\" + 0.009*\"1633\" + 0.009*\"durarono\" + 0.009*\"belloza\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Bcpo8WQ2SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}