{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FwlEnNvQJNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AZPes5RSYG2",
        "colab_type": "text"
      },
      "source": [
        "# 0. Preliminary step to get sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRiBwgFTg2p",
        "colab_type": "text"
      },
      "source": [
        "This preliminary step is reproducing Lorella's workflow Python file:\n",
        "https://i-lab.public.data.uu.nl/vault-ocex/ChroniclItaly%20-%20Italian%20American%20newspapers%20corpus%20from%201898%20to%201920%5B1529330521%5D/original/\n",
        "I just added a folder \"data_1\" to keep all files in one folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK81EVJim0Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir 'data1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O_c4pYzXsP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base URL\n",
        "chronam = 'https://chroniclingamerica.loc.gov/'\n",
        "\n",
        "# Chronicling America search results\n",
        "results = 'https://chroniclingamerica.loc.gov/search/pages/results/?date1=1880&date2=1920&searchType=advanced&language=ita&sequence=1&lccn=2012271201&lccn=sn85066408&lccn=sn85055164&lccn=sn85054967&lccn=sn88064299&lccn=sn84037024&lccn=sn84037025&lccn=sn86092310&proxdistance=5&state=California&state=District+of+Columbia&state=Massachusetts&state=Pennsylvania&state=Piedmont&state=Vermont&state=West+Virginia&rows=100&ortext=&proxtext=&phrasetext=&andtext=&dateFilterType=yearRange&page=11&sort=date'\n",
        "\n",
        "# Count to keep track of downloaded files\n",
        "count = 0\n",
        "\n",
        "# Gets search results in JSON format\n",
        "results_json = results + '&format=json'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwqRYbydrq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns JSON \n",
        "def get_json(url):\n",
        "    data = requests.get(url)\n",
        "    return(json.loads(data.content))\n",
        "    \n",
        "data = get_json(results_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv32dwDndnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cycle through JSON results\n",
        "for page in data['items']:\n",
        "    # Create URL\n",
        "    hit = str(page['id'])\n",
        "    seed = hit + 'ocr.txt'\n",
        "    download_url = chronam + seed\n",
        " \n",
        "    # Create file name\n",
        "    file_name = download_url.replace('/', '_')\n",
        "    file_name = 'data1/' + file_name[41:]\n",
        "    \n",
        "    # Download .txt of the page\n",
        "    urllib.request.urlretrieve(download_url, str(file_name))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUvuIpuzT8ke",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTOrDXSYUIc3",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Grouping all texts files\n",
        "A dataframe is first created to keep individual files at their initial state, and the name of each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u857KfY9WXN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FNWdVJT1v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of the file names\n",
        "files_list = os.listdir('data1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lLD0OpiXv5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#insert file names into a df\n",
        "sources = pd.DataFrame(files_list, columns=['file_name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PgOwPnqCfAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to read the content of the text files\n",
        "def readTxtContent(fileName):\n",
        "  with open('data1/' + fileName, 'r') as file:\n",
        "    return ' ' + file.read().replace('\\n', ' ') + ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEVIKHziQv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding a column to the dataframe containing file content\n",
        "sources['file_content'] = sources['file_name'].apply(lambda x: readTxtContent(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ffdAsOXQSBe",
        "colab_type": "code",
        "outputId": "75c1abdd-308f-416b-a1c6-622c2dc894f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for verification purposes later, count the nr of characters for each content\n",
        "sources['file_len'] = sources['file_content'].apply(lambda x: len(x))\n",
        "sources['file_len'].sum()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwTwsGsED8uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variable containing all texts together\n",
        "corpus = ''\n",
        "for i in range(len(sources)):\n",
        "  corpus += sources['file_content'][i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBu87u_SSM-9",
        "colab_type": "code",
        "outputId": "4e8eab0b-6bff-4bfc-a1df-517732d2f452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check length\n",
        "len(corpus)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1779770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybCetXHi9m-y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.2 Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-8sf7RayKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "acf665bf-1dc1-40c2-c314-6a6e3ab72712"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZjtQSrV_lX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuation and lower case (then depending on user input, leave the possibility to do one or the other)\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "tokenized_corpus = [w.lower() for w in tokens if w.isalnum()]\n",
        "# lower case, remove punctuation and only keep words that have more than 3 letters\n",
        "tokenized_corpus = [w.lower() for w in tokens if (w.isalnum() and len(w) > 3 )]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IrR5gbba5ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show list of default Italian stopwords\n",
        "# stopwords.words('italian')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo3hn7mWbCtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add custom stop word\n",
        "ital_stopwords = stopwords.words('italian')\n",
        "# to append list of words added by user: ital_stopwords.extend(user_input)\n",
        "# to remove words: ital_stopwords.remove(user_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXh4tcgbGg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"stopwords.words('italian')\" can be replaced by a custom list input by the user\n",
        "tokenized_corpus_without_sw = [w for w in tokenized_corpus if not w in stopwords.words('italian')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKs7aXWLbVWo",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyl0C-xkbLiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euUXP7UHbaN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize with needed language\n",
        "stemmer = SnowballStemmer(\"italian\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8fTjp5bean",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_corpus = [stemmer.stem(w) for w in tokenized_corpus_without_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKQAqemcgYz",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8M804chYgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatize is available in multiple languages in Spacy and not in NLTK (only English)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QAPblFUwf3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.it import Italian\n",
        "it_nlp = Italian()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUGXN6NLpcHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I tested spacy Italian lemmatization functionality below, and I'm not very convinced by the result\n",
        "# test = it_nlp('Salvini: \"Resteremo in aula finché vedremo i fatti\". Il pd Orlando: \"Le banche non stanno facendo il loro lavoro\"')\n",
        "# print([token.lemma_ for token in test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JewnrBc6cmV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe with corpus\n",
        "corpus_df = pd.DataFrame(tokenized_corpus_without_sw, columns=['tokens'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYwtxl3XiMFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add column with lemmatized version\n",
        "corpus_df['lemmatized_token'] = corpus_df['tokens'].apply(lambda x: [token.lemma_ for token in it_nlp(x)] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8TRLnFPK6p4",
        "colab_type": "text"
      },
      "source": [
        "# 2. Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PjHRUzLDbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kibSiMo0PxBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [d.split() for d in tokenized_corpus_without_sw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnwCJGqHPrTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbMCT1sRmY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqlYyvDZSer6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import models\n",
        "model = models.LdaModel(corpus, id2word=dictionary, num_topics=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ZdxEeOTf6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "9e39b1ef-1c97-49c6-a1ff-bdd332c45aa5"
      },
      "source": [
        "model.print_topics()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.011*\"roma\" + 0.009*\"gran\" + 0.009*\"governo\" + 0.007*\"voti\" + 0.007*\"giornale\" + 0.007*\"parigi\" + 0.006*\"tempo\" + 0.006*\"generale\" + 0.006*\"essere\" + 0.006*\"ogni\"'),\n",
              " (1,\n",
              "  '0.014*\"dispaccio\" + 0.009*\"egli\" + 0.009*\"dopo\" + 0.008*\"fatto\" + 0.008*\"anni\" + 0.007*\"quali\" + 0.007*\"anno\" + 0.007*\"luglio\" + 0.007*\"giorni\" + 0.007*\"francisco\"'),\n",
              " (2,\n",
              "  '0.017*\"italia\" + 0.014*\"stato\" + 0.009*\"italiano\" + 0.009*\"oggi\" + 0.009*\"tutte\" + 0.007*\"altri\" + 0.007*\"quando\" + 0.007*\"avere\" + 0.006*\"città\" + 0.006*\"nulla\"'),\n",
              " (3,\n",
              "  '0.010*\"ministro\" + 0.009*\"york\" + 0.007*\"italiani\" + 0.006*\"parte\" + 0.006*\"stati\" + 0.006*\"grande\" + 0.005*\"direttore\" + 0.005*\"pubblica\" + 0.005*\"napoli\" + 0.004*\"numero\"'),\n",
              " (4,\n",
              "  '0.012*\"telegrafico\" + 0.008*\"daily\" + 0.007*\"italiana\" + 0.007*\"sempre\" + 0.006*\"dice\" + 0.005*\"quotidiano\" + 0.005*\"malgrado\" + 0.004*\"grandi\" + 0.004*\"chiesa\" + 0.004*\"membro\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    }
  ]
}